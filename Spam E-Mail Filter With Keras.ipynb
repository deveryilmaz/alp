{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deveryilmaz/alp/blob/master/Spam%20E-Mail%20Filter%20With%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Tensorflow with GPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ7E58sOBcC5",
        "colab_type": "text"
      },
      "source": [
        "# Spam E-Mail Filter With Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etSUspWMBh3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD, RMSprop\n",
        "from numpy import zeros\n",
        "from random import shuffle\n",
        "from random import seed\n",
        "from matplotlib import pyplot\n",
        "import time\n",
        "\n",
        "#from keras.layers import Dropout #overfitting - ezberleme önüne geçmek için eklendi\n",
        "\n",
        "ilk = time.asctime(time.localtime())\n",
        "print('Başlama Zamanı: ', ilk)\n",
        "'''\n",
        "Read the file with the training and test data and return\n",
        "it as two separate lists. Both lists will be shuffled before\n",
        "they are returned.\n",
        "'''\n",
        "def read_lines():\n",
        "    train_lines = []\n",
        "    test_lines = []\n",
        "    current_lines = []\n",
        "\n",
        "    with open('SpamHamData.txt') as f: \n",
        "        for line in f.readlines(): \n",
        "            if line.startswith('# Test data', 0):\n",
        "                train_lines = current_lines\n",
        "                current_lines = test_lines\n",
        "            elif line.startswith('#', 0):\n",
        "                '''\n",
        "                Ignore comment lines\n",
        "                '''\n",
        "            elif line == '\\n':\n",
        "                '''\n",
        "                Ignore empty lines\n",
        "                '''\n",
        "            else:\n",
        "                current_lines.append(line)\n",
        "\n",
        "    test_lines = current_lines\n",
        "    \n",
        "    seed(1337)\n",
        "    shuffle(train_lines)\n",
        "    shuffle(test_lines)\n",
        "\n",
        "    print('Read training lines: ', len(train_lines))\n",
        "    print('Read test lines: ', len(test_lines))\n",
        "\n",
        "    return train_lines, test_lines\n",
        "\n",
        "'''\n",
        "Take a list of lines from the original input file (train or test), remove\n",
        "paragraphs and line breaks and split into label and data by using the comma \n",
        "as divider. Return as two separate lists preserving the sort order.\n",
        "'''\n",
        "def split_lines(lines):\n",
        "    data = []\n",
        "    labels = []\n",
        "    maxtokens = 0\n",
        "    for line in lines:\n",
        "        label_part, data_part = line.replace('<p>','').replace('</p>','').replace('\\n', '').split(',')\n",
        "        data.append(data_part)\n",
        "        labels.append(label_part)\n",
        "        if (len(data_part)>maxtokens):\n",
        "            maxtokens=len(data_part)\n",
        "\n",
        "    print('maxlen ', maxtokens)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "'''\n",
        "While processing the data with Keras each original text will converted\n",
        "to a list of indices. These indices point to words in a dictionary\n",
        "of all words contained in the training data. We convert this to a binary\n",
        "matrix. The value 1 in the matrix says that a word (x in the matrix) is\n",
        "contained in a given text (y in the matrix)\n",
        "'''\n",
        "def vectorize_sequences(sequences, dimension=4000):\n",
        "    results = zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "'''\n",
        "The label vectorization is quite simple:\n",
        "  the value 1 is for spam,\n",
        "  the value 0 is form ham\n",
        "'''\n",
        "def vectorize_labels(labels):\n",
        "    results = zeros(len(labels))\n",
        "    for i, label in enumerate(labels):\n",
        "        if (label.lower() == 'spam'):\n",
        "            results[i] = 1\n",
        "    return results\n",
        "\n",
        "def test_predict(model, testtext, expected_label):\n",
        "    testtext_list = []\n",
        "    testtext_list.append(testtext)\n",
        "    testtext_sequence = tokenizer.texts_to_sequences(testtext_list)\n",
        "    x_testtext = vectorize_sequences(testtext_sequence)\n",
        "    prediction = model.predict(x_testtext)[0][0]\n",
        "    \n",
        "    print(\"Tahmin: %.3f\" % prediction, 'Expected ', expected_label)\n",
        "\n",
        "    if prediction > 0.5:\n",
        "        if expected_label == 'Spam':\n",
        "            return True\n",
        "    else:\n",
        "        if expected_label == 'Ham':\n",
        "            return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "# Start script\n",
        "\n",
        "# First split train lines from test lines\n",
        "train_lines, test_lines = read_lines()\n",
        "\n",
        "# Split data from label for each line\n",
        "train_data_raw, train_labels_raw = split_lines(train_lines)\n",
        "test_data_raw, test_labels_raw = split_lines(test_lines)\n",
        "\n",
        "# Use Keras Tokenizer to vectorize text: \n",
        "# fit_on_texts will setup the internal vocabulary using all words\n",
        "# from the training data and attaching indices to them\n",
        "# texts_to_sequences will transform each text into sequence of\n",
        "# integer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data_raw)#fit_on_text, eğitim verilerinde kullanılan tüm kelimelerin bir sözlüğünü ve her bir kelime için bir indeks numarası oluşturacaktır.\n",
        "train_data_seq = tokenizer.texts_to_sequences(train_data_raw)#texts_to_sequences her metni tam sayı sırasına dönüştürür\n",
        "test_data_seq = tokenizer.texts_to_sequences(test_data_raw)\n",
        "\n",
        "# Finally the integer sequenes are converted to a binary (numpy)\n",
        "# matrix where rows are for the text lines, columns are for\n",
        "# the words. 1 = word is inside text, 0 = word is not inside\n",
        "x_train = vectorize_sequences(train_data_seq, 4000)\n",
        "print('Lines of training data: ', len(x_train))\n",
        "x_test = vectorize_sequences(test_data_seq, 4000)\n",
        "print('Lines of test data: ', len(x_test))\n",
        "\n",
        "# The labels are also converted to a binary vector.\n",
        "# 1 means spam, 0 means ham\n",
        "y_train = vectorize_labels(train_labels_raw)\n",
        "print('Lines of training results: ', len(y_train))\n",
        "y_test = vectorize_labels(test_labels_raw)\n",
        "print('Lines of test results: ', len(y_test))\n",
        "\n",
        "# Now we build the Keras model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='tanh', input_shape=(4000,)))\n",
        "model.add(Dropout(0.5)) #overfitting - ezberleme önüne geçmek için eklendi\n",
        "model.add(layers.Dense(16, activation='sigmoid'))\n",
        "model.add(Dropout(0.5)) #overfitting - ezberleme önüne geçmek için eklendi\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "r = RMSprop(lr=0.0001, rho=0.9)#lr düştükçe tahminimiz düşecektir.\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "#optimizer= 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam' \n",
        "#loss=sparse_categorical_crossentropy, binary_crossentropy, mean_squared_error\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train,y_train,epochs=64,batch_size=100,validation_split=0.5)\n",
        "\n",
        "\n",
        "\n",
        "epochs=range(1, 65)\n",
        "history_dict = history.history\n",
        "\n",
        "# summarize history for accuracy\n",
        "pyplot.plot(history.history['acc'])\n",
        "pyplot.plot(history.history['val_acc'])\n",
        "pyplot.title('model accuracy')\n",
        "pyplot.ylabel('accuracy')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['training', 'validation'], loc='lower right')\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.clf()\n",
        "acc_values = history_dict['acc']\n",
        "val_acc_values = history_dict['val_acc']\n",
        "pyplot.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "pyplot.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "pyplot.title('Training and validation accuracy')\n",
        "pyplot.xlabel('Epochs')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# summarize history for loss\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('model loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['training', 'validation'], loc='upper right')\n",
        "pyplot.show()\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "pyplot.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "pyplot.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "pyplot.title('Training and validation loss')\n",
        "pyplot.xlabel('Epochs')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "\n",
        "\n",
        "results = model.evaluate(x_test, y_test)\n",
        "print(model.metrics_names)\n",
        "print('Test result: ', results)\n",
        "\n",
        "# Manual test over all test records\n",
        "correct = 0\n",
        "wrong = 0\n",
        "for input_text, expected_label in zip(test_data_raw, test_labels_raw):\n",
        "    if test_predict(model, input_text, expected_label):\n",
        "        correct = correct + 1\n",
        "    else:\n",
        "        wrong = wrong + 1\n",
        "\n",
        "print('Predictions correct ', correct, ', wrong ', wrong)\n",
        "son = time.asctime(time.localtime())\n",
        "print('Bitirme zamanı Zamanı: ', son)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}